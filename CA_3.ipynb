{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a deep neural network\n",
    "\n",
    "In this task we consider the following problem as our optimization problem:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\underset{W_1,W_2,w_3}{\\text{minimize}} \\; \\frac{1}{N} \\sum_{i\\in [N]} ||w_3s(W_2s(W_1x_i)-y_i)||_2^2\n",
    "\\end{equation*}\n",
    "\n",
    "where s is sigmoid function with the formula s = 1/(1+exp(-x)). We also can add the our choice of regulizer. As we can see from the above formula, we can implement our potimization problem with a neural network with two hidden layers and one output layer. All the activation functions of our neural network will be sigmoid function. Since the size of $W_3$ and $W_2$ and $W_1$ is not specified we can set it by our choice. We can also add $L_2$ normalizer to our neural network. \n",
    "\n",
    "In the first part of our code we will implement our neural network with different otimizers (GD, SGD, SVRG, SAG). We will use Keras library and python. \n",
    "\n",
    "## 1. Data preparation\n",
    "\n",
    "The data sets are same as CA1 and CA2. Before starting to work with the data sets we will discuss that how we use the data sets. As we do not have any condition on the way we use these data sets, we can choose only some prefered feture set of the data sets. We prepare our data sets following the bellow steps.\n",
    "\n",
    "- The firs two feilds of \"Individual household electric power consumption\" data set are timestamp and date. We can simply eliminate these fields since the goal of this assignment is not finding a very accurate model for this data set (The goal is comparing different solvers using neural network for same data sets). The other professional way of using these columns is to convert these columns to integer values of time and using them.\n",
    "\n",
    "- Both data sets have missing values. There are different ways of handling missing values. In this assignment similar to CA2 we just replace these missing values with zeros. \n",
    "\n",
    "- In this assignment since we use neural network with sigmoid as our activation function, we will use normalization preprocessing of data to prevent our network from satiate (sigmoid function is usually sensitive to range of our input space and it will saturate very fast with respect to range of input values and in these case our otimozer will become very slow, so we can set the mean value of our input to 0 and standard deviation to 1). For preprocessing, Scikit library has different functions.  The two functions that we can use is preprocessing.scale and preprocessing.normalize which do same action on data with different ways (normalizing along one axis or whole axis of data). We will use preprocessing.scale which normalize the input space over all axis of input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahab/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of data set is: (2075259, 9)\n",
      "The first three rows of data set:\n",
      "          Date      Time Global_active_power Global_reactive_power  Voltage  \\\n",
      "0  16/12/2006  17:24:00               4.216                 0.418  234.840   \n",
      "1  16/12/2006  17:25:00               5.360                 0.436  233.630   \n",
      "\n",
      "  Global_intensity Sub_metering_1 Sub_metering_2  Sub_metering_3  \n",
      "0           18.400          0.000          1.000            17.0  \n",
      "1           23.000          0.000          1.000            16.0  \n",
      "The last three rows of data set:\n",
      "                Date      Time Global_active_power Global_reactive_power  \\\n",
      "2075257  26/11/2010  21:01:00               0.934                     0   \n",
      "2075258  26/11/2010  21:02:00               0.932                     0   \n",
      "\n",
      "        Voltage Global_intensity Sub_metering_1 Sub_metering_2  Sub_metering_3  \n",
      "2075257   239.7              3.8              0              0             0.0  \n",
      "2075258  239.55              3.8              0              0             0.0  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"household_power_consumption.txt\",\";\")\n",
    "\n",
    "print(\"The size of data set is:\",data.shape)\n",
    "print(\"The first three rows of data set:\\n\",data.head(2))\n",
    "print(\"The last three rows of data set:\\n\",data.tail(2))\n",
    "\n",
    "X_ = pd.DataFrame(data.iloc[:,2:6], columns=[\"Global_active_power\",\"Global_reactive_power\",\"Voltage\",\"Global_intensity\"])\n",
    "Y_ = pd.DataFrame(data.iloc[:,7], columns=[\"Sub_metering_2\"])\n",
    "\n",
    "X_ = X_.replace({'?':0})\n",
    "Y_ = Y_.replace({'?':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 train and test sets\n",
    "\n",
    "Befor starting the training we need to split our data to train and test. In traditional learning algorithms, people usually use 70%/30% train/test or 60%/20%/20% train/dev/test set. Nowadays, because of having large data sets, people usually use 98%/1%/1% train/dev/test set. In this assignment we used the traditional way of spliting data into train and test with the ratio of 70%/30% of train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set (X):  (2520, 4)\n",
      "Size of train set (Y):  (2520, 1)\n",
      "Size of test set (X):  (1080, 4)\n",
      "Size of test set (Y):  (1080, 1)\n"
     ]
    }
   ],
   "source": [
    "X_features = X_.columns\n",
    "Y_features = Y_.columns\n",
    "number_of_samples = 3600\n",
    "XY = pd.concat([X_[X_features], Y_[Y_features]], axis=1).iloc[0:number_of_samples,:]\n",
    "\n",
    "# Split XY into training set and test set of equal size\n",
    "train, test = train_test_split(XY, test_size = 0.30)\n",
    "# Sort the train and test sets after index (which became unsorted through sampling)\n",
    "train = train.sort_index(axis=0)\n",
    "test = test.sort_index(axis=0)\n",
    "\n",
    "# Extract X,Y components from test and train sets\n",
    "X_train = train[X_features].astype(float); X_test = test[X_features].astype(float)\n",
    "Y_train = train[Y_features].astype(float); Y_test = test[Y_features].astype(float)\n",
    "\n",
    "print(\"Size of train set (X): \",X_train.shape)\n",
    "print(\"Size of train set (Y): \",Y_train.shape)\n",
    "print(\"Size of test set (X): \",X_test.shape)\n",
    "print(\"Size of test set (Y): \",Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining metrics for comparing the models and methods\n",
    "\n",
    "In this part we will introduce our metric that it gives us the possiblity of comparing the methods and the accuracy of our models when we predict our target variable for test set. We will use Normalized Mean Absolute Error.\n",
    "\n",
    "We use following formula to compute this error:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{\\bar{y}}(\\frac{1}{m}\\sum\\limits_{i=1}^m|y_i-\\hat{y_i}|)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nmae(y,yhat):\n",
    "    y = np.array(y).reshape(y.shape[0],1)\n",
    "    yhat = np.array(yhat).reshape(yhat.shape[0],1)\n",
    "    result = np.sum(np.abs(y-yhat))/(y.shape[0]*np.mean(y))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "y = np.array([1,2,3,4])\n",
    "yhat = np.array([2,1,3,4])\n",
    "print(nmae(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining a Naive and baseline models\n",
    "\n",
    "In this section we will specify two other models which will be our baseline and naive estimators (just to know and understand and have feeling about the prediction and modeling environment). Our Naive estimator can be the mean value of our target variable. We can use random forest model as our base line model. We can compare the results of our neural network with the results of these two models to assess the accuracy of our model (although the goal of this assignment is not to find a good estimator, we can just be sure that we are not wasting time with wrong model). However, when we are using neural networks we can not necceraly be shure that in our first attempt we will have a good model as our baseline or even our naive method. We usually need to tune different hyper-parameters in our neural network. Such as the architecture which is simply the size of our matrices in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive(y):\n",
    "    return np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_metering_2    2.099206\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "naive_estimator = naive(Y_train)\n",
    "print(naive_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=120, n_jobs=-1)\n",
    "regressor.fit(X_train, Y_train)\n",
    "test_pred = regressor.predict(X_test)\n",
    "train_pred = regressor.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test NMAE is 0.93\n",
      "The train NMAE is 0.35\n",
      "The naive NMAE is 1.53\n"
     ]
    }
   ],
   "source": [
    "test_nmae = nmae(Y_test,test_pred)\n",
    "train_nmae = nmae(Y_train,train_pred)\n",
    "naive_values = np.full((Y_train.shape[0],1),naive_estimator)\n",
    "naive_nmae = nmae(Y_train,naive_values)\n",
    "print(\"The test NMAE is %.2f\" %test_nmae)\n",
    "print(\"The train NMAE is %.2f\" %train_nmae)\n",
    "print(\"The naive NMAE is %.2f\" %naive_nmae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAEYCAYAAABBfQDEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a12cd4710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "N_size = min(X_test.shape[0],number_of_samples)\n",
    "baseline_value = Y_train.mean()\n",
    "baseline_array = np.zeros(N_size)\n",
    "for i in range(0,N_size):\n",
    "    baseline_array[i] = baseline_value\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 4))\n",
    "line = plt.plot(np.linspace(0, N_size-1, N_size),Y_test[0:N_size], color='y')\n",
    "baseline = plt.plot(np.linspace(0, N_size-1, N_size),baseline_array[0:N_size], color='g')\n",
    "predict_line = plt.plot(np.linspace(0, N_size-1, N_size),test_pred[0:N_size], color='red')\n",
    "\n",
    "font_size = 14\n",
    "plt.xlabel('Sample index', fontsize=font_size)\n",
    "plt.ylabel('Power Consumption', fontsize=font_size)\n",
    "plt.legend(('Measured', 'Estimated values','Naive estimatore'), loc = 'upper right', shadow=False, fancybox=True, fontsize=font_size)\n",
    "plt.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=font_size)\n",
    "plt.rcParams['savefig.dpi'] = 200\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network with different optimizers\n",
    "\n",
    "Now we are ready to implement our model with different optimizers. In the first part we will just do the first try to achive the acceptable results incomparision to our baseline and naive model. In the second part we will discuss about the different parameters.\n",
    "\n",
    "\n",
    "### 3.1 In the first step we will start with a very simple neural network with GD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature standarization: mean=0, variance=1\n",
    "from sklearn import preprocessing\n",
    "X_train = preprocessing.scale(X_train)\n",
    "X_test = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2520, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "epsilon = 1e-9\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * epsilon\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[  1.78862847e-09   4.36509851e-10   9.64974681e-11  -1.86349270e-09\n",
      "   -2.77388203e-10]\n",
      " [ -3.54758979e-10  -8.27414815e-11  -6.27000677e-10  -4.38181690e-11\n",
      "   -4.77218030e-10]\n",
      " [ -1.31386475e-09   8.84622380e-10   8.81318042e-10   1.70957306e-09\n",
      "    5.00336422e-11]\n",
      " [ -4.04677415e-10  -5.45359948e-10  -1.54647732e-09   9.82367434e-10\n",
      "   -1.10106763e-09]]\n",
      "b1 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "W2 = [[ -1.18504653e-09  -2.05649899e-10   1.48614836e-09   2.36716267e-10]\n",
      " [ -1.02378514e-09  -7.12993200e-10   6.25244966e-10  -1.60513363e-10]\n",
      " [ -7.68836350e-10  -2.30030722e-10   7.45056266e-10   1.97611078e-09]]\n",
      "b2 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A)+b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    activation_cache = {\"Z\": x}\n",
    "    A = x * (x > 0)\n",
    "    return A,activation_cache\n",
    "\n",
    "def drelu(x):\n",
    "    return 1. * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x):\n",
    "    activation_cache = {\"Z\": x}\n",
    "    A = x\n",
    "    A[x < 0] = 0.01 * A[x < 0]\n",
    "    return A\n",
    "\n",
    "def dlrelu(x):\n",
    "    dx = np.ones_like(x,np.float64)\n",
    "    dx[x < 0] = 0.01\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1/(1+np.exp((-1)*z))\n",
    "    \n",
    "    activation_cache = {\"Z\": z}\n",
    "    \n",
    "    return s,activation_cache\n",
    "def dsigmoid(z):\n",
    "\n",
    "    s = z*(1-z)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    s = (np.exp(z)+np.exp((-1)*z))/(np.exp(z)+np.exp((-1)*z))\n",
    "    \n",
    "    activation_cache = {\"Z\": z}\n",
    "    \n",
    "    return s,activation_cache\n",
    "def dtanh(z):\n",
    "\n",
    "    s = 1-(z*z)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(z):\n",
    "    s=z\n",
    "    activation_cache = {\"Z\":z}\n",
    "    return s,activation_cache\n",
    "\n",
    "def dlinear(z):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == \"lrelu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = lrelu(Z)\n",
    "    elif activation == \"linear\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = linear(Z)\n",
    "    elif activation == \"tanh\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = tanh(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"linear\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    \n",
    "    #print (\"AL.shape:\",AL.shape,\" X.shape[1]:\",X.shape[1])\n",
    "    \n",
    "    #***assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    diff = (A2-Y)**2\n",
    "    cost = np.sum(diff)/m\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.333333333333\n"
     ]
    }
   ],
   "source": [
    "AL = np.array([1,2,3]).reshape(1,3)\n",
    "Y = np.array([1,2,2]).reshape(1,3)\n",
    "cost = compute_cost(AL,Y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    dZ = dA*dsigmoid(activation_cache['Z'])\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, activation_cache):\n",
    "    dZ = dA*drelu(activation_cache['Z'])\n",
    "    return dZ\n",
    "\n",
    "def lrelu_backward(dA, activation_cache):\n",
    "    dZ = dA*dlrelu(activation_cache['Z'])\n",
    "    return dZ\n",
    "\n",
    "def line_backward(dA, activation_cache):\n",
    "    dZ = dA*dlinear(activation_cache['Z'])\n",
    "    return dZ\n",
    "\n",
    "def tanh_backward(dA, activation_cache):\n",
    "    dZ = dA*dtanh(activation_cache['Z'])\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = (np.sum(dZ,axis=1,keepdims=True)/m)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    elif activation == \"lrelu\":\n",
    "        dZ = lrelu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        #print (\"dZ.shape:\",dZ.shape)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "        #print (\"dZ.shape:\",dZ.shape)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = 2*np.sum(AL-Y)/m\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    #grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"linear\")\n",
    "\n",
    "    linear_cache, activation_cache = current_cache\n",
    "    A_prev, W, b = linear_cache\n",
    "    \n",
    "    grads[\"dA\" + str(L)] = dAL\n",
    "    grads[\"dW\" + str(L)] = 2 * np.dot((AL-Y),A_prev.T)/m\n",
    "    grads[\"db\" + str(L)] = dAL\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "    \n",
    "        current_cache = caches[l]\n",
    "        #dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"lrelu\")\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"sigmoid\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(1, L+1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - np.multiply(learning_rate , grads[\"dW\"+str(l)])\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - np.multiply(learning_rate , grads[\"db\"+str(l)])\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters_SVRG(parameters, grads, grads_tilda, total_grads, learning_rate,N):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(1, L+1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - np.multiply(learning_rate , grads[\"dW\"+str(l)]-grads_tilda[\"dW\"+str(l)]+total_grads[\"dW\"+str(l)]/N)\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - np.multiply(learning_rate , grads[\"db\"+str(l)]-grads_tilda[\"db\"+str(l)]+total_grads[\"db\"+str(l)]/N)\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters_SAG(parameters,grads, SAG_grads, random_index, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(1, L+1):\n",
    "        if (l == random_index):\n",
    "            parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - np.multiply(learning_rate , grads[\"dW\"+str(l)])\n",
    "            parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - np.multiply(learning_rate , grads[\"db\"+str(l)])\n",
    "        else:\n",
    "            parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - np.multiply(learning_rate , SAG_grads[l][\"dW\"+str(l)])\n",
    "            parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - np.multiply(learning_rate , SAG_grads[l][\"db\"+str(l)])\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "random_index = int(random.randint(1,X_train.shape[1]))\n",
    "print(random_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [4, 2, 2, 1] #  5-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, epoch_length = 50, print_cost=False, solver=\"GD\"):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    parameters_tilda = initialize_parameters_deep(layers_dims)\n",
    "    SAG_grads = [dict() for x in range(X.shape[1])]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    if (solver == \"GD\"):\n",
    "        for i in range(0, num_iterations):\n",
    "\n",
    "            # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "            AL, caches = L_model_forward(X, parameters)\n",
    "\n",
    "            # Compute cost.\n",
    "            cost = compute_cost(AL, Y)\n",
    "\n",
    "            # Backward propagation.\n",
    "            grads = L_model_backward(AL, Y, caches)\n",
    "\n",
    "            # Update parameters.\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "\n",
    "    elif (solver == \"SGD\"):\n",
    "        for i in range(0, num_iterations):\n",
    "            random_index = int(random.randint(1,X.shape[1]-1))\n",
    "            random_sample_X = (X[:,random_index]).reshape(X.shape[0],1)\n",
    "            random_sample_Y = (Y[:,random_index]).reshape(1,1)\n",
    "            \n",
    "            # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "            AL, caches = L_model_forward(random_sample_X, parameters)\n",
    "\n",
    "            # Compute cost.\n",
    "            cost = compute_cost(AL, random_sample_Y)\n",
    "\n",
    "            # Backward propagation.\n",
    "            grads = L_model_backward(AL, random_sample_Y, caches)\n",
    "\n",
    "            # Update parameters.\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "\n",
    "        \n",
    "    elif (solver == \"SVRG\"):\n",
    "        for i in range(0, num_iterations):\n",
    "            # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "            AL, caches = L_model_forward(X, parameters_tilda)\n",
    "\n",
    "            # Compute cost.\n",
    "            cost = compute_cost(AL, Y)\n",
    "\n",
    "            # Backward propagation. Tilda gradian f\n",
    "            total_grads = L_model_backward(AL, Y, caches)\n",
    "            \n",
    "            for j in range(0,epoch_length):\n",
    "                random_index = int(random.randint(1,X.shape[1]-1))\n",
    "                random_sample_X = (X[:,random_index]).reshape(X.shape[0],1)\n",
    "                random_sample_Y = (Y[:,random_index]).reshape(1,1)\n",
    "\n",
    "                # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "                AL, caches = L_model_forward(random_sample_X, parameters)\n",
    "\n",
    "                # Compute cost.\n",
    "                cost = compute_cost(AL, random_sample_Y)\n",
    "\n",
    "                # Backward propagation.\n",
    "                grads = L_model_backward(AL, random_sample_Y, caches)\n",
    "                \n",
    "                # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "                AL_tilda, caches_tilda = L_model_forward(random_sample_X, parameters_tilda)\n",
    "\n",
    "                # Compute cost.\n",
    "                cost_tilda = compute_cost(AL_tilda, random_sample_Y)\n",
    "\n",
    "                # Backward propagation.\n",
    "                grads_tilda = L_model_backward(AL_tilda, random_sample_Y, caches_tilda)\n",
    "\n",
    "                # Update parameters.\n",
    "                parameters = update_parameters_SVRG(parameters, grads, grads_tilda, total_grads, learning_rate,X.shape[1])\n",
    "            \n",
    "            parameters_tilda = parameters\n",
    "            \n",
    "    elif (solver == \"SAG\"):\n",
    "        #Computing all gradients for all samples\n",
    "        for i in range(X.shape[1]):\n",
    "            \n",
    "            sample_X = (X[:,i]).reshape(X.shape[0],1)\n",
    "            sample_Y = (Y[:,i]).reshape(1,1)\n",
    "\n",
    "            # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "            AL, caches = L_model_forward(sample_X, parameters)\n",
    "\n",
    "            # Compute cost.\n",
    "            cost = compute_cost(AL, sample_Y)\n",
    "\n",
    "            # Backward propagation.\n",
    "            grads = L_model_backward(AL, sample_Y, caches)\n",
    "            SAG_grads[i] = grads\n",
    "        \n",
    "        \n",
    "        \n",
    "        for j in range(0,epoch_length):\n",
    "            random_index = int(random.randint(1,X.shape[1]-1))\n",
    "            random_sample_X = (X[:,random_index]).reshape(X.shape[0],1)\n",
    "            random_sample_Y = (Y[:,random_index]).reshape(1,1)\n",
    "\n",
    "            # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "            AL, caches = L_model_forward(random_sample_X, parameters)\n",
    "\n",
    "            # Compute cost.\n",
    "            cost = compute_cost(AL, random_sample_Y)\n",
    "            grads = L_model_backward(AL, sample_Y, caches)\n",
    "            \n",
    "            # Update parameters.\n",
    "            parameters = update_parameters_SAG(parameters, grads, SAG_grads, random_index, learning_rate)\n",
    "\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = L_layer_model(X_train.T, np.array(Y_train).T, layers_dims, solver = \"SAG\", learning_rate = 0.001, num_iterations = 500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    A2, cache = L_model_forward(X,parameters)\n",
    "    predictions = A2 \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1080)\n"
     ]
    }
   ],
   "source": [
    "test_predicted_values = predict(parameters, X_test.T)\n",
    "train_predicted_values = predict(parameters, X_train.T)\n",
    "print(test_predicted_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test NMAE is 1.01\n",
      "The train NMAE is 1.02\n",
      "The naive NMAE is 1.53\n"
     ]
    }
   ],
   "source": [
    "test_nmae = nmae(Y_test,test_predicted_values.T)\n",
    "train_nmae = nmae(Y_train,train_predicted_values.T)\n",
    "naive_values = np.full((Y_train.shape[0],1),naive_estimator)\n",
    "naive_nmae = nmae(Y_train,naive_values)\n",
    "print(\"The test NMAE is %.2f\" %test_nmae)\n",
    "print(\"The train NMAE is %.2f\" %train_nmae)\n",
    "print(\"The naive NMAE is %.2f\" %naive_nmae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Comparing different models\n",
    "\n",
    "In this part we will run our model and compare them based on training time and accuracy.\n",
    "\n",
    "#### 3.2.1 Gradient Descent (GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time is 0.13\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "parameters = L_layer_model(X_train.T, np.array(Y_train).T, layers_dims, solver = \"GD\", learning_rate = 0.001, num_iterations = 500, print_cost = True)\n",
    "end = time.time()\n",
    "\n",
    "train_time = end - start \n",
    "print(\"Training time is %.2f\" %train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    A2, cache = L_model_forward(X,parameters)\n",
    "    predictions = A2 \n",
    "\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1080)\n"
     ]
    }
   ],
   "source": [
    "test_predicted_values = predict(parameters, X_test.T)\n",
    "train_predicted_values = predict(parameters, X_train.T)\n",
    "print(test_predicted_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test NMAE is 1.24\n",
      "The train NMAE is 1.26\n",
      "The naive NMAE is 1.53\n"
     ]
    }
   ],
   "source": [
    "test_nmae = nmae(Y_test,test_predicted_values.T)\n",
    "train_nmae = nmae(Y_train,train_predicted_values.T)\n",
    "naive_values = np.full((Y_train.shape[0],1),naive_estimator)\n",
    "naive_nmae = nmae(Y_train,naive_values)\n",
    "print(\"The test NMAE is %.2f\" %test_nmae)\n",
    "print(\"The train NMAE is %.2f\" %train_nmae)\n",
    "print(\"The naive NMAE is %.2f\" %naive_nmae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time is 0.06\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "parameters = L_layer_model(X_train.T, np.array(Y_train).T, layers_dims, solver = \"SGD\", learning_rate = 0.001, num_iterations = 500, print_cost = True)\n",
    "end = time.time()\n",
    "\n",
    "train_time = end - start \n",
    "print(\"Training time is %.2f\" %train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    A2, cache = L_model_forward(X,parameters)\n",
    "    predictions = A2 \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1080)\n"
     ]
    }
   ],
   "source": [
    "test_predicted_values = predict(parameters, X_test.T)\n",
    "train_predicted_values = predict(parameters, X_train.T)\n",
    "print(test_predicted_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test NMAE is 1.24\n",
      "The train NMAE is 1.26\n",
      "The naive NMAE is 1.53\n"
     ]
    }
   ],
   "source": [
    "test_nmae = nmae(Y_test,test_predicted_values.T)\n",
    "train_nmae = nmae(Y_train,train_predicted_values.T)\n",
    "naive_values = np.full((Y_train.shape[0],1),naive_estimator)\n",
    "naive_nmae = nmae(Y_train,naive_values)\n",
    "print(\"The test NMAE is %.2f\" %test_nmae)\n",
    "print(\"The train NMAE is %.2f\" %train_nmae)\n",
    "print(\"The naive NMAE is %.2f\" %naive_nmae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Stochastic variance reduced gradient (SVRG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time is 5.67\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "parameters = L_layer_model(X_train.T, np.array(Y_train).T, layers_dims, solver = \"SVRG\", learning_rate = 0.001, num_iterations = 500, print_cost = True)\n",
    "end = time.time()\n",
    "\n",
    "train_time = end - start \n",
    "print(\"Training time is %.2f\" %train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    A2, cache = L_model_forward(X,parameters)\n",
    "    predictions = A2 \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1080)\n"
     ]
    }
   ],
   "source": [
    "test_predicted_values = predict(parameters, X_test.T)\n",
    "train_predicted_values = predict(parameters, X_train.T)\n",
    "print(test_predicted_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test NMAE is 1.01\n",
      "The train NMAE is 1.01\n",
      "The naive NMAE is 1.53\n"
     ]
    }
   ],
   "source": [
    "test_nmae = nmae(Y_test,test_predicted_values.T)\n",
    "train_nmae = nmae(Y_train,train_predicted_values.T)\n",
    "naive_values = np.full((Y_train.shape[0],1),naive_estimator)\n",
    "naive_nmae = nmae(Y_train,naive_values)\n",
    "print(\"The test NMAE is %.2f\" %test_nmae)\n",
    "print(\"The train NMAE is %.2f\" %train_nmae)\n",
    "print(\"The naive NMAE is %.2f\" %naive_nmae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Stochastic average gradient (SAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time is 0.25\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "parameters = L_layer_model(X_train.T, np.array(Y_train).T, layers_dims, solver = \"SAG\", learning_rate = 0.001, num_iterations = 500, print_cost = True)\n",
    "end = time.time()\n",
    "\n",
    "train_time = end - start \n",
    "print(\"Training time is %.2f\" %train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    A2, cache = L_model_forward(X,parameters)\n",
    "    predictions = A2 \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1080)\n"
     ]
    }
   ],
   "source": [
    "test_predicted_values = predict(parameters, X_test.T)\n",
    "train_predicted_values = predict(parameters, X_train.T)\n",
    "print(test_predicted_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test NMAE is 1.01\n",
      "The train NMAE is 1.02\n",
      "The naive NMAE is 1.53\n"
     ]
    }
   ],
   "source": [
    "test_nmae = nmae(Y_test,test_predicted_values.T)\n",
    "train_nmae = nmae(Y_train,train_predicted_values.T)\n",
    "naive_values = np.full((Y_train.shape[0],1),naive_estimator)\n",
    "naive_nmae = nmae(Y_train,naive_values)\n",
    "print(\"The test NMAE is %.2f\" %test_nmae)\n",
    "print(\"The train NMAE is %.2f\" %train_nmae)\n",
    "print(\"The naive NMAE is %.2f\" %naive_nmae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "If we compare methods and the results, we can see that the most accurate algorithm with specified architecture for neural network, fixed numbr of epochs and learning rate, is SVRG the next accurate is SAG and GD and last one is SGD. If compare training time we can see that the fastest algorithm is SGD the second one is GD, and the next one is SAG and last one is SVRG.\n",
    "\n",
    "We have to notice that all these results are primary results and we need to tune every thing to get good results. As we know there are different parameters here that we can tunel like the architecture, optimizer paramters (learning rate, epochs, epoch length for SVRG and so on). To tune the parameters very similar to task two we can specify the space of hyper-paramters and select points in this space, then define models with the hyper-parameters and train the models. We select the models with lowest error or highest accuracy of our metric (here we chose NMAE). And we can continue to select best paramters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
